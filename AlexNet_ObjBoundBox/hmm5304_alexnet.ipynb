{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using softmax for image class\n",
    "* Using sigmoid for bound box coordinates (since the values are between 0-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetObjClassBoud:\n",
    "    @staticmethod\n",
    "    def classBranch(inputs, numCategories, fActivation=\"softmax\"):\n",
    "        x = Dense(units = 512)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(numCategories)(x)\n",
    "        x = Activation(fActivation, name=\"class_output\")(x)\n",
    "        \n",
    "        # return class prediction sub-network\n",
    "        return x\n",
    "    \n",
    "    @staticmethod    \n",
    "    def boundBranch(inputs, numBoxCord, fActivation=\"sigmoid\"):\n",
    "        x = Dense(units = 128)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(units = 64)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(units = 32)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(numBoxCord)(x)\n",
    "        x = Activation(fActivation, name=\"bound_output\")(x)\n",
    "        \n",
    "        # return bound prediction sub-network \n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build(imWidth, imHeight, numCategories, numBoxCord):\n",
    "        #initialize the image input shape and channel dimension\n",
    "        inputShape = (imHeight, imWidth, 3) #RBG\n",
    "        chanDim = -1\n",
    "        \n",
    "        # construct alexnet\n",
    "        inputLayer = Input(shape = inputShape)\n",
    "        # 1\n",
    "        x = Conv2D(filters=96, kernel_size=(11, 11), strides=(4,4), padding=\"valid\")(inputLayer)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3), strides=(2,2), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 2\n",
    "        x = Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3), strides=(2,2), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 3\n",
    "        x = Conv2D(filters=384, kernel_size=(3, 3), strides=(1,1), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 4\n",
    "        x = Conv2D(filters=384, kernel_size=(3, 3), strides=(1,1), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 5\n",
    "        x = Conv2D(filters=256, kernel_size=(3, 3), strides=(1,1), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3), strides=(2,2), padding=\"valid\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 6\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(units = 9216)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 7 \n",
    "        x = Dense(units = 4096)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        # 8\n",
    "        x = Dense(units = 4096)(x)\n",
    "        inputs = Activation(\"relu\")(x)\n",
    "        \n",
    "        # Output \n",
    "        # going to pass the layer before the last layer to \n",
    "        # image classifier network and image bound network\n",
    "        # x = Dense(units = 1000)(x)\n",
    "        \n",
    "        imageClassBranch = AlexNetObjClassBoud.classBranch(inputs, numCategories, \n",
    "                                                           fActivation=\"softmax\")\n",
    "        imageBoundBranch = AlexNetObjClassBoud.boundBranch(inputs, numBoxCord, \n",
    "                                                           fActivation=\"sigmoid\")\n",
    "        \n",
    "        # create final model with 2 seperate outputs\n",
    "        model = Model(inputs = inputLayer, outputs = [imageClassBranch, imageBoundBranch], \n",
    "                     name = \"alexnetclassbound\")\n",
    "        \n",
    "        # return the network\n",
    "        return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub = pd.read_csv(\"/storage/home/hmm5304/scratch/OpenImagesV5/train_data_sub_down.txt\", sep ='\\t')\n",
    "validation_data_sub = pd.read_csv(\"/storage/home/hmm5304/scratch/OpenImagesV5/validation_data_sub_down.txt\", sep ='\\t')\n",
    "test_data_sub = pd.read_csv(\"/storage/home/hmm5304/scratch/OpenImagesV5/test_data_sub_down.txt\", sep ='\\t')\n",
    "train_data_sub=train_data_sub[train_data_sub['ImageFound']==1]\n",
    "validation_data_sub=validation_data_sub[validation_data_sub['ImageFound']==1]\n",
    "test_data_sub=test_data_sub[test_data_sub['ImageFound']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bus           2826\n",
       "Airplane      2720\n",
       "Bird          2708\n",
       "Truck         2707\n",
       "Person        2694\n",
       "Cat           2674\n",
       "Desk          2672\n",
       "Motorcycle    2668\n",
       "Laptop        2661\n",
       "Chair         2266\n",
       "Bicycle       1784\n",
       "Name: LabelDescription, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sub['LabelDescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person        1157\n",
       "Airplane       432\n",
       "Bird           320\n",
       "Truck          181\n",
       "Cat            116\n",
       "Chair           73\n",
       "Bicycle         55\n",
       "Motorcycle      52\n",
       "Bus             49\n",
       "Desk            16\n",
       "Laptop          15\n",
       "Name: LabelDescription, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_sub['LabelDescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person        2668\n",
       "Airplane      1314\n",
       "Bird          1043\n",
       "Truck          522\n",
       "Cat            357\n",
       "Chair          221\n",
       "Bicycle        203\n",
       "Bus            164\n",
       "Motorcycle     126\n",
       "Laptop          61\n",
       "Desk            46\n",
       "Name: LabelDescription, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_sub['LabelDescription'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Source</th>\n",
       "      <th>LabelName</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>XMin</th>\n",
       "      <th>XMax</th>\n",
       "      <th>YMin</th>\n",
       "      <th>YMax</th>\n",
       "      <th>IsOccluded</th>\n",
       "      <th>IsTruncated</th>\n",
       "      <th>IsGroupOf</th>\n",
       "      <th>IsDepiction</th>\n",
       "      <th>IsInside</th>\n",
       "      <th>LabelDescription</th>\n",
       "      <th>OriginalURL</th>\n",
       "      <th>Rotation</th>\n",
       "      <th>ImageFound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75424f2fadf1c6c9</td>\n",
       "      <td>activemil</td>\n",
       "      <td>/m/04_sv</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074375</td>\n",
       "      <td>0.901875</td>\n",
       "      <td>0.318894</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>https://c2.staticflickr.com/1/627/21126611322_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a151e217ac86652a</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568720</td>\n",
       "      <td>0.883886</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.879173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Person</td>\n",
       "      <td>https://c3.staticflickr.com/6/5074/5844189273_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81af87fff7c21972</td>\n",
       "      <td>activemil</td>\n",
       "      <td>/m/0199g</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102927</td>\n",
       "      <td>0.868744</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>0.846875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Bicycle</td>\n",
       "      <td>https://farm3.staticflickr.com/3709/8906157384...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4da1efdefe9a7b6</td>\n",
       "      <td>activemil</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456481</td>\n",
       "      <td>0.562963</td>\n",
       "      <td>0.692593</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Person</td>\n",
       "      <td>https://farm7.staticflickr.com/714/20772066013...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29af45a96c1b05ce</td>\n",
       "      <td>activemil</td>\n",
       "      <td>/m/07r04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.146250</td>\n",
       "      <td>0.746875</td>\n",
       "      <td>0.270501</td>\n",
       "      <td>0.941427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Truck</td>\n",
       "      <td>https://c7.staticflickr.com/1/185/455840246_cd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ImageID     Source  LabelName  Confidence      XMin      XMax  \\\n",
       "0  75424f2fadf1c6c9  activemil   /m/04_sv           1  0.074375  0.901875   \n",
       "1  a151e217ac86652a     xclick  /m/01g317           1  0.568720  0.883886   \n",
       "2  81af87fff7c21972  activemil   /m/0199g           1  0.102927  0.868744   \n",
       "3  f4da1efdefe9a7b6  activemil  /m/01g317           1  0.456481  0.562963   \n",
       "4  29af45a96c1b05ce  activemil   /m/07r04           1  0.146250  0.746875   \n",
       "\n",
       "       YMin      YMax  IsOccluded  IsTruncated  IsGroupOf  IsDepiction  \\\n",
       "0  0.318894  0.971429           0            0          0            0   \n",
       "1  0.432432  0.879173           0            0          0            0   \n",
       "2  0.026250  0.846875           0            0          0            0   \n",
       "3  0.692593  0.963889           0            0          0            0   \n",
       "4  0.270501  0.941427           0            0          0            0   \n",
       "\n",
       "   IsInside LabelDescription  \\\n",
       "0         0       Motorcycle   \n",
       "1         0           Person   \n",
       "2         0          Bicycle   \n",
       "3         0           Person   \n",
       "4         0            Truck   \n",
       "\n",
       "                                         OriginalURL  Rotation  ImageFound  \n",
       "0  https://c2.staticflickr.com/1/627/21126611322_...       0.0         1.0  \n",
       "1  https://c3.staticflickr.com/6/5074/5844189273_...       0.0         1.0  \n",
       "2  https://farm3.staticflickr.com/3709/8906157384...       0.0         1.0  \n",
       "3  https://farm7.staticflickr.com/714/20772066013...       NaN         1.0  \n",
       "4  https://c7.staticflickr.com/1/185/455840246_cd...       NaN         1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub = pd.concat([train_data_sub, pd.get_dummies(train_data_sub.LabelDescription)], axis=1)\n",
    "validation_data_sub = pd.concat([validation_data_sub, pd.get_dummies(validation_data_sub.LabelDescription)], axis=1)\n",
    "test_data_sub = pd.concat([test_data_sub, pd.get_dummies(test_data_sub.LabelDescription)], axis=1)\n",
    "\n",
    "train_data_sub['imagePath'] = \"/storage/home/hmm5304/scratch/OpenImagesV5/data/train/\" + train_data_sub['ImageID'] + \".jpg\"\n",
    "validation_data_sub['imagePath'] = \"/storage/home/hmm5304/scratch/OpenImagesV5/data/validation/\" + validation_data_sub['ImageID'] + \".jpg\"\n",
    "test_data_sub['imagePath'] = \"/storage/home/hmm5304/scratch/OpenImagesV5/data/test/\" + test_data_sub['ImageID'] + \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Motorcycle', 'Person', 'Bicycle', 'Truck', 'Desk', 'Bus', 'Bird',\n",
       "       'Airplane', 'Chair', 'Cat', 'Laptop'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_classes = train_data_sub['LabelDescription'].unique()\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize run parameters\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (227,227,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training data\n",
    "# class labels and bounding box\n",
    "train_data = []\n",
    "train_class = []\n",
    "train_bb = []\n",
    "\n",
    "train_data_sub = train_data_sub.reset_index()\n",
    "for i in range(train_data_sub.shape[0]-1):\n",
    "    image = cv2.imread(train_data_sub['imagePath'][i])\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # rotate image\n",
    "    if train_data_sub['Rotation'][i] == 90:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif train_data_sub['Rotation'][i] == 180:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    elif train_data_sub['Rotation'][i] == 270:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    image = img_to_array(image)\n",
    "    train_data.append(image)\n",
    "    train_class.append(np.array(train_data_sub.loc[i,unique_classes]))\n",
    "    train_bb.append(np.array(train_data_sub.loc[i,['XMin','XMax','YMin','YMax']]))\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] and convert to np array\n",
    "train_data = np.array(train_data, dtype=\"float\") / 255.0\n",
    "train_class = np.array(train_class)\n",
    "train_bb = np.array(train_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training data\n",
    "# class labels and bounding box\n",
    "validation_data = []\n",
    "validation_class = []\n",
    "validation_bb = []\n",
    "\n",
    "validation_data_sub = validation_data_sub.reset_index()\n",
    "for i in range(validation_data_sub.shape[0]-1):\n",
    "    image = cv2.imread(validation_data_sub['imagePath'][i])\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # rotate image\n",
    "    if validation_data_sub['Rotation'][i] == 90:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif validation_data_sub['Rotation'][i] == 180:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    elif validation_data_sub['Rotation'][i] == 270:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    image = img_to_array(image)\n",
    "    validation_data.append(image)\n",
    "    validation_class.append(np.array(validation_data_sub.loc[i,unique_classes]))\n",
    "    validation_bb.append(np.array(validation_data_sub.loc[i,['XMin','XMax','YMin','YMax']]))\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] and convert to np array\n",
    "validation_data = np.array(validation_data, dtype=\"float\") / 255.0\n",
    "validation_class = np.array(validation_class)\n",
    "validation_bb = np.array(validation_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training data\n",
    "# class labels and bounding box\n",
    "test_data = []\n",
    "test_class = []\n",
    "test_bb = []\n",
    "\n",
    "test_data_sub = test_data_sub.reset_index()\n",
    "for i in range(test_data_sub.shape[0]-1):\n",
    "    image = cv2.imread(test_data_sub['imagePath'][i])\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # rotate image\n",
    "    if test_data_sub['Rotation'][i] == 90:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif test_data_sub['Rotation'][i] == 180:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    elif test_data_sub['Rotation'][i] == 270:\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "        image = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    image = img_to_array(image)\n",
    "    test_data.append(image)\n",
    "    test_class.append(np.array(test_data_sub.loc[i,unique_classes]))\n",
    "    test_bb.append(np.array(test_data_sub.loc[i,['XMin','XMax','YMin','YMax']]))\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] and convert to np array\n",
    "test_data = np.array(test_data, dtype=\"float\") / 255.0\n",
    "test_class = np.array(test_class)\n",
    "test_bb = np.array(test_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using categorical crossentropy as loss for image class (the output is one-hot encoded)\n",
    "* Using mean squared error as loss for bound box, since it is a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize alexnet\n",
    "model = AlexNetObjClassBoud.build(IMAGE_DIMS[1], IMAGE_DIMS[0], unique_classes.size, 4)\n",
    "\n",
    "# define loss function for each category\n",
    "# class_output bound_output\n",
    "losses = {\"class_output\":\"categorical_crossentropy\", \"bound_output\":\"mean_squared_error\"}\n",
    "lossWeights = {\"class_output\": 1.0, \"bound_output\": 1.0}\n",
    "\n",
    "# initialize the optimizer and compile the model\n",
    "opt = Adam(learning_rate=INIT_LR)\n",
    "model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"alexnetclassbound\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 227, 227, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 55, 55, 96)   34944       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 55, 55, 96)   0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 27, 27, 96)  0           ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 27, 27, 96)   0           ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 23, 23, 256)  614656      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 23, 23, 256)  0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 11, 11, 256)  0          ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 11, 11, 256)  0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 9, 9, 384)    885120      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 9, 9, 384)    0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 7, 7, 384)    1327488     ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 7, 7, 384)    0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 5, 5, 256)    884992      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 5, 5, 256)    0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 2, 2, 256)   0           ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 2, 2, 256)    0           ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 1024)         0           ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 9216)         9446400     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 9216)         0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 4096)         37752832    ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4096)         0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 4096)         16781312    ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4096)         0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 128)          524416      ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 128)          0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64)           8256        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 64)           0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 512)          2097664     ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 32)           2080        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 512)          0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32)           0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 11)           5643        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 4)            132         ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " class_output (Activation)      (None, 11)           0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " bound_output (Activation)      (None, 4)            0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70,365,935\n",
      "Trainable params: 70,365,935\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the network\n",
    "# train the network to perform multi-output classification\n",
    "model_history = model.fit(x=train_data, y={\"class_output\": train_class, \"bound_output\": train_bb},\n",
    "    validation_data=(validation_data, {\"class_output\": validation_class, \"bound_output\": validation_bb}),\n",
    "    epochs=EPOCHS, batch_size=BS, verbose=1)\n",
    "\n",
    "# save the model \n",
    "model.save(\"/storage/home/hmm5304/scratch/OpenImagesV5/alexnet_ObjClassBoud_v2\", save_format=\"h5\")\n",
    "\n",
    "# save history\n",
    "with open('/storage/home/hmm5304/scratch/OpenImagesV5/trainHistoryDictWBS', 'wb') as file_pi:\n",
    "    pickle.dump(model_history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSG: plot the total loss, class loss, and bound loss...\")\n",
    "# plot the total loss, class loss, and bound loss\n",
    "lossNames = [\"loss\", \"class_output_loss\", \"bound_output_loss\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "\n",
    "# loop over the loss names\n",
    "for (i, l) in enumerate(lossNames):\n",
    "    # plot the loss for both the training and validation data\n",
    "    title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "    ax[i].set_title(title)\n",
    "    ax[i].set_xlabel(\"Epoch #\")\n",
    "    ax[i].set_ylabel(\"Loss\")\n",
    "    ax[i].plot(np.arange(0, EPOCHS), model_history.history[l], label=l)\n",
    "    ax[i].plot(np.arange(0, EPOCHS), model_history.history[\"val_\" + l],\n",
    "        label=\"val_\" + l)\n",
    "    ax[i].legend()\n",
    "\n",
    "# save the losses figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/storage/home/hmm5304/scratch/OpenImagesV5/alexnet_ObjClassBoud_v2_losses.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure for the accuracies\n",
    "accuracyNames = [\"class_output_accuracy\", \"bound_output_accuracy\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "# loop over the accuracy names\n",
    "for (i, l) in enumerate(accuracyNames):\n",
    "    # plot the accuracy for both the training and validation data\n",
    "    title = \"Accuracy for {}\".format(l) \n",
    "    ax[i].set_title(title)\n",
    "    ax[i].set_xlabel(\"Epoch #\")\n",
    "    ax[i].set_ylabel(\"Accuracy\")\n",
    "    ax[i].plot(np.arange(0, EPOCHS), model_history.history[l], label=l)\n",
    "    ax[i].plot(np.arange(0, EPOCHS), model_history.history[\"val_\" + l],\n",
    "        label=\"val_\" + l)\n",
    "    ax[i].legend()\n",
    "# save the accuracies figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/storage/home/hmm5304/scratch/OpenImagesV5/alexnet_ObjClassBoud_v2_accuracy.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"/storage/home/hmm5304/scratch/OpenImagesV5/alexnet_ObjClassBoud_v2\", custom_objects={\"tf\": tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6724, 227, 227, 3)\n",
      "(6724, 11)\n",
      "(6724, 4)\n"
     ]
    }
   ],
   "source": [
    "# initialize training data\n",
    "# class labels and bounding box\n",
    "test_data = np.load(\"/storage/home/hmm5304/scratch/OpenImagesV5/data/numpy_data/test_data.npy\", allow_pickle=True)\n",
    "test_data = np.asarray(test_data).astype('float32')\n",
    "print(test_data.shape)\n",
    "test_class = np.load(\"/storage/home/hmm5304/scratch/OpenImagesV5/data/numpy_data/test_class.npy\", allow_pickle=True)\n",
    "test_class = np.asarray(test_class).astype('float32')\n",
    "print(test_class.shape)\n",
    "test_bb = np.load(\"/storage/home/hmm5304/scratch/OpenImagesV5/data/numpy_data/test_bb.npy\", allow_pickle=True)\n",
    "test_bb = np.asarray(test_bb).astype('float32')\n",
    "print(test_bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG: Evaluating model of test images...\n",
      "211/211 [==============================] - 63s 295ms/step - loss: 2.4066 - class_output_loss: 2.3646 - bound_output_loss: 0.0420 - class_output_accuracy: 0.0244 - bound_output_accuracy: 0.4920\n",
      "MSG: Classifying test images...\n",
      "MSG: Save test images prediction...\n"
     ]
    }
   ],
   "source": [
    "# predict on test set\n",
    "print(\"MSG: Evaluating model of test images...\")\n",
    "model.evaluate(x = test_data, y = {\"class_output\": test_class, \"bound_output\": test_bb})\n",
    "\n",
    "print(\"MSG: Classifying test images...\")\n",
    "(classProbability, bboxCoordinates) = model.predict(test_data)\n",
    "\n",
    "print(\"MSG: Save test images prediction...\")\n",
    "np.save(\"/storage/home/hmm5304/scratch/OpenImagesV5/data/numpy_data/classProbability_v2.npy\", classProbability)\n",
    "np.save(\"/storage/home/hmm5304/scratch/OpenImagesV5/data/numpy_data/bboxCoordinates_v2.npy\", bboxCoordinates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
